prastogi@r5n07 ~/projects/eldemo/third_party/nnPUlearning Sun Jun 03 14:31:40 master
$ python train.py -g 0 --preset exp-mnist
/opt/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
(61000, 1, 28, 28)
training:(61000, 1, 28, 28)
test:(10000, 1, 28, 28)
prior: 0.4915333333333333
loss: sigmoid
batchsize: 30000
model: <class 'model.MultiLayerPerceptron'>
beta: 0.0
gamma: 1.0

epoch       train/nnPU/error  test/nnPU/error  train/uPU/error  test/uPU/error  elapsed_time
1           0.508467          0.4493           0.508467         0.4493          6.55474
2           0.492204          0.2921           0.492204         0.2921          7.9313
3           0.492204          0.1708           0.492204         0.1708          9.28113
4           0.492204          0.1121           0.492204         0.1121          10.5686
5           0.492204          0.0884           0.492204         0.0884          11.8621
6           0.492204          0.0729           0.492204         0.0784          13.2064
7           0.492204          0.0767           0.492204         0.0731          14.5604
8           0.492204          0.0656           0.492204         0.0809          15.8197
9           0.492204          0.0559           0.492204         0.0938          17.117
10          0.492204          0.0606           0.492204         0.098           18.4354
11          0.492204          0.0664           0.492204         0.1119          19.6941
12          0.492204          0.0721           0.492204         0.1378          20.9608
13          0.492204          0.0622           0.492204         0.1431          22.3225
14          0.492204          0.0589           0.492204         0.1777          23.591
15          0.492204          0.0598           0.492204         0.1871          24.858
16          0.492204          0.062            0.492204         0.2439          26.1863
17          0.492204          0.0664           0.492204         0.2154          27.4767
18          0.492204          0.0592           0.492204         0.2822          28.8327
19          0.492204          0.0584           0.492204         0.3217          30.1441
20          0.492204          0.0556           0.492204         0.2766          31.4486
21          0.492204          0.0539           0.492204         0.355           32.7375
22          0.492204          0.0552           0.492204         0.3428          34.0325
23          0.492204          0.0533           0.492204         0.3471          35.319
24          0.492204          0.0553           0.492204         0.386           36.6409
25          0.492204          0.0551           0.492204         0.3661          37.935
26          0.492204          0.0587           0.492204         0.3677          39.3089
27          0.492204          0.0587           0.491533         0.4238          40.665
28          0.492204          0.0618           0.492204         0.3858          41.9931
29          0.492204          0.0707           0.491533         0.3906          43.3292
30          0.492204          0.0657           0.491533         0.4166          44.5958
31          0.492204          0.0624           0.491533         0.4323          46.1132
32          0.492204          0.0778           0.491533         0.4133          47.4026
33          0.492204          0.0715           0.491533         0.4223          48.7256
34          0.492204          0.0542           0.491533         0.4142          50.0022
35          0.492204          0.0507           0.491533         0.4352          51.379
36          0.492204          0.0551           0.491533         0.4353          52.6915
37          0.492204          0.0806           0.491533         0.4283          54.0224
38          0.492204          0.0543           0.491533         0.4679          55.4472
39          0.492204          0.0546           0.491533         0.4406          56.8379
40          0.504334          0.0747           0.491533         0.4423          58.1966
41          0.504334          0.0883           0.491533         0.4657          59.4973
42          0.492204          0.0582           0.491533         0.4447          60.7678
43          0.492204          0.0887           0.491533         0.451           62.0154
44          0.492204          0.0527           0.491533         0.4573          63.2616
45          0.492204          0.0973           0.491533         0.4318          64.5623
46          0.492204          0.0716           0.491533         0.4543          65.8849
47          0.492204          0.0632           0.491533         0.4465          67.1297
48          0.492204          0.064            0.491533         0.4596          68.3831
     total [########################..........................] 49.18%
this epoch [#########.........................................] 18.03%
       100 iter, 49 epoch / 100 epochs
       inf iters/sec. Estimated time to finish: 0:00:00.
49          0.492204          0.0917           0.491533         0.4622          69.6329
50          0.492204          0.0868           0.491533         0.4577          70.8789
51          0.492204          0.0716           0.491533         0.4752          72.2784
52          0.492204          0.0688           0.491533         0.4667          73.5688
53          0.492204          0.0754           0.491533         0.4476          74.8205
54          0.492204          0.0599           0.491533         0.469           76.0757
55          0.492204          0.0455           0.491533         0.467           77.3336
56          0.492204          0.044            0.491533         0.4437          78.6104
57          0.492204          0.0451           0.491533         0.4862          79.9443
58          0.492204          0.052            0.491533         0.4455          81.2213
59          0.492204          0.0478           0.491533         0.4769          82.527
60          0.492204          0.0488           0.491533         0.4632          83.8626
61          0.492204          0.0549           0.491533         0.4696          85.3945
62          0.492204          0.0583           0.491533         0.4561          86.7252
63          0.492204          0.0704           0.492204         0.4534          88.0199
64          0.492204          0.0678           0.491533         0.4727          89.2803
65          0.492204          0.0675           0.491533         0.4822          90.5431
66          0.492204          0.0792           0.491533         0.4462          91.8034
67          0.492204          0.0471           0.491533         0.4793          93.0655
68          0.492204          0.0553           0.491533         0.4371          94.392
69          0.492204          0.0663           0.491533         0.4637          95.7022
70          0.492204          0.0647           0.491533         0.4425          96.958
71          0.492204          0.08             0.492204         0.4476          98.2076
72          0.492204          0.0984           0.491533         0.4796          99.4644
73          0.492204          0.135            0.491533         0.4554          100.778
74          0.492204          0.1138           0.491533         0.4648          102.071
75          0.492204          0.1065           0.491533         0.4709          103.433
76          0.492204          0.0602           0.491533         0.4226          104.714
77          0.492204          0.0624           0.491533         0.4559          105.986
78          0.492204          0.0682           0.491533         0.4607          107.272
79          0.492204          0.113            0.491533         0.4711          108.622
80          0.492204          0.0723           0.491533         0.462           109.909
81          0.492204          0.0655           0.491533         0.4741          111.197
82          0.492204          0.1274           0.491533         0.4825          112.48
83          0.492204          0.097            0.491533         0.4811          113.762
84          0.492204          0.0775           0.491533         0.4809          115.077
85          0.492204          0.0845           0.491533         0.4712          116.336
86          0.492204          0.0692           0.491533         0.4531          117.594
87          0.492204          0.0712           0.491533         0.4604          118.85
88          0.492204          0.0807           0.491533         0.442           120.118
89          0.492204          0.0658           0.491533         0.4638          121.381
90          0.492204          0.0598           0.491533         0.4645          122.688
91          0.492204          0.0573           0.491533         0.4791          124.184
92          0.492204          0.0602           0.491533         0.4775          125.44
93          0.492204          0.0812           0.491533         0.4514          126.701
94          0.492204          0.0647           0.491533         0.4635          127.96
95          0.492204          0.0667           0.491533         0.4572          129.265
96          0.492204          0.0864           0.491533         0.4533          130.522
97          0.492204          0.0753           0.491533         0.4574          131.793
     total [#################################################.] 98.36%
this epoch [##################................................] 36.07%
       200 iter, 98 epoch / 100 epochs
    1.5765 iters/sec. Estimated time to finish: 0:00:02.114381.
98          0.492204          0.0739           0.491533         0.4684          133.063
99          0.492204          0.0736           0.491533         0.4751          134.328
100         0.492204          0.0785           0.491533         0.466           135.654
